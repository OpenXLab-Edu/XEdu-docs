<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>图片分类模型MobileNet &mdash; OpenXLabEdu  documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="author" title="About these documents" href="../../about.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="prev" title="图片分类模型LeNet-5" href="lenet5.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> OpenXLabEdu
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">目录</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../about.html">关于XEdu</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mmedu.html">MMEdu入门</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../basenn.html">BaseNN入门</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../baseml.html">BaseML入门</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../scitech_tools.html">相关科创工具</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../support_resources.html">学习支持和资源获取</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../dl_library.html">深度学习知识库</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../parameter_explain.html">深度学习训练参数详解</a></li>
<li class="toctree-l2"><a class="reference internal" href="../dataset_introduction.html">经典数据集介绍</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../network_introduction.html">经典网络模型介绍</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="lenet5.html">图片分类模型LeNet-5</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">图片分类模型MobileNet</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id1">介绍</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id2">特点：深度可分离卷积</a></li>
<li class="toctree-l4"><a class="reference internal" href="#linear-bottleneck">特点：Linear Bottleneck</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id3">网络结构</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id4">优点</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id5">使用领域</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id6">参考文献</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">OpenXLabEdu</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../dl_library.html">深度学习知识库</a> &raquo;</li>
          <li><a href="../network_introduction.html">经典网络模型介绍</a> &raquo;</li>
      <li>图片分类模型MobileNet</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/dl_library/net/mobilenet.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="mobilenet">
<h1>图片分类模型MobileNet<a class="headerlink" href="#mobilenet" title="Permalink to this headline"></a></h1>
<p><a class="reference external" href="https://arxiv.org/abs/1801.04381">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a></p>
<section id="id1">
<h2>介绍<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h2>
<p>MobileNetV2是Google对MobileNetV1提出的改良版本。MobileNets系列的本身初衷是” <strong>for Mobile Vision Applications</strong>”。也就是提出一个轻量化的卷积网络模型，可以显著减少计算量和参数量，同时保持较高的准确率来提升效率，这样的模型是如此的的轻量化甚至可以在移动模型上进行训练。
其主要创新性是采用了<strong>深度可分离卷积</strong>的思想，也就是把标准的卷积过程拆分成<strong>深度卷积</strong>和<strong>逐层卷积</strong>两个过程，大大减少了参数量。</p>
</section>
<section id="id2">
<h2>特点：深度可分离卷积<a class="headerlink" href="#id2" title="Permalink to this headline"></a></h2>
<p>传统的标准卷积操作在M通道图像的基础上，准备得到N个特征图，使用的基础卷积核尺寸为$(W \cdot H)$。
每一个特征图对应一个filter，需要用M个小卷积核进行卷积，即每一个filter为$(W \cdot H \cdot M)$个参数，而这样的操作重复N次，就得到了N个特征图，即总参数为$(W \cdot H \cdot M \cdot N)$。</p>
<p><img alt="image" src="images/dl_library/%E4%BC%A0%E7%BB%9F%E5%8D%B7%E7%A7%AF.png" /></p>
<p>而为了简化这一计算过程，采用两次计算：
我们首先在深度方向进行卷积（深度卷积、逐通道卷积），先改变特征图的大小而不改变特征图的数量，即输入是M个通道，输出也是M个通道，先不涉及通道数的变化：</p>
<p><img alt="avatar" src="images/dl_library/dw%E5%8D%B7%E7%A7%AF.png" /></p>
<p>可以看到，每一个filter都是单层，这一步的参数量为$(W \cdot H \cdot M)$.</p>
<p>此时特征图大小已经改变，只需要从M维映射到N维即可。我们需要对每个点进行卷积（逐点卷积），添加N个filters，每个filter尺寸为$(1 \times 1 \cdot M)$，也就是我们常说的加权平均：</p>
<p><img alt="avatar" src="images/dl_library/pw%E5%8D%B7%E7%A7%AF.png" /></p>
<p>可以看到，逐点卷积通过对老通道作N次“加权平均数”的运算，得到了N个新的特征图。这一步的参数量为$(1 \times 1 \cdot M \cdot N)$</p>
<p>这两次计算的总参数量为$1 \times 1 \cdot M \cdot N + W \cdot H \cdot M = M \cdot (N + W \cdot H)$
参数量是传统卷积的几分之一呢？答案是：
$$
\frac{M \cdot (N + W \cdot H)}{W \cdot H \cdot M \cdot N} = \frac{1}{W \cdot H } + \frac{1}{N}
$$
在网络中的大部分层中，我们使用的是$3 \times 3$的卷积核，而N一般是几十到几百的数量级，因此一般参数可以缩减到传统方法的九分之一。而在实际测试中，原论文指出，准确度仅有1.1%的减少，但是参数量缩减到约七分之一：</p>
<p><img alt="avatar" src="images/dl_library/v1%E5%AF%B9%E6%AF%94.PNG" /></p>
</section>
<section id="linear-bottleneck">
<h2>特点：Linear Bottleneck<a class="headerlink" href="#linear-bottleneck" title="Permalink to this headline"></a></h2>
<p>MobileNetV1使用的激活函数为Relu，这个函数也是深度学习常用的激活函数之一。在MobileNetV2原始论文中，MobileNet团队指出，这个激活函数在较低维度无法保证信息的相对完整，会造成一定程度的丢失，下面这个例子有助于理解：
有一条螺旋线$X$在二维空间内，我们使用某矩阵$T$将其映射到高维空间内，之后进行Relu操作，再使用某矩阵$T$的逆矩阵$T^{-1}$将其降维回二维。也就是说进行了一个：
$$
X’ = T^{-1}(Relu(T \cdot X))
$$
的操作。如果没有任何信息损失，$X’$和$X$就会是完全一致的，论文作者给出的结果是：</p>
<p><img alt="avatar" src="../../_images/jw.PNG" /></p>
<p>可以很直接的看出，维度越高，Relu造成的损失越小，而在低维度的情况下，信息失真很严重，这样会造成很多卷积核的死亡（权重为0）。
于是作者在某些层舍弃了Relu，采用了线性的变化函数，进行了一次升维的操作，作者将其称之为”Linear Bottleneck”。此部分的细节我们不做深入阐述，这一步的作用就是给原网络升维，从而避免了很多卷积核的死亡。</p>
</section>
<section id="id3">
<h2>网络结构<a class="headerlink" href="#id3" title="Permalink to this headline"></a></h2>
<p>至此我们给出网络结构：</p>
<p><img alt="avatar" src="images/dl_library/v2%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84.PNG" /></p>
<p>可以从网络结构中看到上面我们描述的深度卷积层（conv2d）和逐点卷积层（conv2d 1x1）。网络结构肉眼可见的简洁和清晰，而效果也不俗。</p>
</section>
<section id="id4">
<h2>优点<a class="headerlink" href="#id4" title="Permalink to this headline"></a></h2>
<p>总结一下优点，最大的优点就是创新性的可分离卷积带来的大量参数减少，从而导致网络的轻量化。此外，V2比起V1，还增加了Linear Bottleneck机制来避免卷积核的死亡，从而提高参数利用率。</p>
</section>
<section id="id5">
<h2>使用领域<a class="headerlink" href="#id5" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>算力相对较低的移动平台的模型部署</p></li>
<li><p>减少大型项目的运行时间同时保留较高的准确率</p></li>
</ul>
</section>
<section id="id6">
<h2>参考文献<a class="headerlink" href="#id6" title="Permalink to this headline"></a></h2>
<div class="highlight-bibtex notranslate"><div class="highlight"><pre><span></span><span class="nc">@misc</span><span class="p">{</span><span class="nl">howard2017mobilenets</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications}</span><span class="p">,</span><span class="w"> </span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Andrew G. Howard and Menglong Zhu and Bo Chen and Dmitry Kalenichenko and Weijun Wang and Tobias Weyand and Marco Andreetto and Hartwig Adam}</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2017}</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{1704.04861}</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CV}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
<span class="nc">@misc</span><span class="p">{</span><span class="nl">sandler2019mobilenetv2</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="na">title</span><span class="p">=</span><span class="s">{MobileNetV2: Inverted Residuals and Linear Bottlenecks}</span><span class="p">,</span><span class="w"> </span>
<span class="w">      </span><span class="na">author</span><span class="p">=</span><span class="s">{Mark Sandler and Andrew Howard and Menglong Zhu and Andrey Zhmoginov and Liang-Chieh Chen}</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="na">year</span><span class="p">=</span><span class="s">{2019}</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="na">eprint</span><span class="p">=</span><span class="s">{1801.04381}</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="na">archivePrefix</span><span class="p">=</span><span class="s">{arXiv}</span><span class="p">,</span><span class="w"></span>
<span class="w">      </span><span class="na">primaryClass</span><span class="p">=</span><span class="s">{cs.CV}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="lenet5.html" class="btn btn-neutral float-left" title="图片分类模型LeNet-5" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021-2022 OpenXLabEdu.All Rights Reserved.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>