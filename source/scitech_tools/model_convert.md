# 模型转换与部署

## 一、简介

用XEdu系列工具训练的模型，只能运行在安装了XEdu环境的电脑吗？怎么样能方便地将训练好的AI模型部署到不同的硬件设备上呢？XEdu提供了模型转换与部署的工具，我们将通过以下几个方面来进行详细讲解：

为什么需要模型转换和部署？

如何进行模型转换？

如何进行模型部署？

模型在不同硬件设备上的性能表现。

## 二、为什么需要模型转换和部署？

模型转换：为了让训练好的模型能在不同框架间流转，通常需要将模型从训练框架转换为推理框架。这样可以在各种硬件设备上部署模型，提高模型的通用性和实用性。

模型部署：将训练好的模型应用到实际场景中，如手机、开发板等。模型部署需要解决环境配置、运行效率等问题。

### 为什么要进行模型转换

用MMEdu训练的模型，只能运行在安装了MMEdu环境的电脑吗？

不。借助模型转换，MMEdu（包括BaseNN）训练的模型都可以转换为ONNX模型，然后部署在很多常见的电脑（开源硬件）上。

模型转换是为了模型能在不同框架间流转。在实际应用时，模型转换几乎都用于工业部署，负责模型从训练框架到部署侧推理框架的连接。 这是因为随着深度学习应用和技术的演进，训练框架和推理框架的职能已经逐渐分化。 分布式、自动求导、混合精度……训练框架往往围绕着易用性，面向设计算法的研究员，以研究员能更快地生产高性能模型为目标。 硬件指令集、预编译优化、量化算法……推理框架往往围绕着硬件平台的极致优化加速，面向工业落地，以模型能更快执行为目标。由于职能和侧重点不同，没有一个深度学习框架能面面俱到，完全一统训练侧和推理侧，而模型在各个框架内部的表示方式又千差万别，所以模型转换就被广泛需要了。

**概括：** 训练框架大，塞不进两三百块钱买的硬件设备中，推理框架小，能在硬件设备上安装。要把训练出的模型翻译成推理框架能读懂的语言，才能在硬件设备上运行

### 为什么要进行模型量化

模型量化是指将深度学习模型中的参数、激活值等数据转化为更小的数据类型（通常是8位整数或者浮点数），以达到模型大小减小、计算速度加快、内存占用减小等优化目的的技术手段。模型量化有以下几个优点：减小模型大小、加速模型推理、减少内存占用等。因此，模型量化可以帮助提高深度学习模型的效率和性能，在实际应用中具有重要的价值和意义。

**概括：** 对模型采用合适的量化，能在对准确率忽略不计的情况下，让模型更小、更快、更轻量。比如原先168 MB的模型量化后大小变为了42.6 MB，推理速度提高了两倍。

### 为什么要进行多模态交互

多模态交互是指利用多个感知通道（例如语音、图像、触觉、姿态等）进行交互的技术。多模态交互在人机交互、智能交通、健康医疗、教育培训等领域都有广泛的应用、在提高交互效率、用户体验、解决单模态限制和实现智能化交互等方面具有重要的作用和价值。

**概括：** 多模态交互给你的AI作品加点创客料


### 什么是推理框架

深度学习推理框架是一种让深度学习算法在实时处理环境中提高性能的框架。常见的有<a href="https://github.com/microsoft/onnxruntime">ONNXRuntime</a>、<a href="https://github.com/Tencent/ncnn">NCNN</a>、<a href="https://github.com/NVIDIA/TensorRT">TensorRT</a>、<a href="https://github.com/openvinotoolkit/openvino">OpenVINO</a>等。

ONNXRuntime是微软推出的一款推理框架，支持多种运行后端包括CPU，GPU，TensorRT，DML等，是对ONNX模型最原生的支持。

NCNN是腾讯公司开发的移动端平台部署工具，一个为手机端极致优化的高性能神经网络前向计算框架。NCNN仅用于推理，不支持学习。

**值得注意的是，包括Pytorch、Tensorflow，以及国内的百度PaddlePaddle、华为的MindSpore等主流的深度学习框架都开发了工具链来回应这个Why。我们采用业界主流的方法，以更高代码封装度的形式来解决这一问题。接下来，且听我们对利用`XEdu`进行`How：怎么做`的流程娓娓道来。**

## How：怎么做

总结一下Why中的回应，在软件工程中，部署指把开发完毕的软件投入使用的过程，包括环境配置、软件安装等步骤。类似地，对于深度学习模型来说，模型部署指让训练好的模型在特定环境中运行的过程。相比于软件部署，模型部署会面临更多的难题：

1. 运行模型所需的环境难以配置。深度学习模型通常是由一些框架编写，比如 PyTorch、TensorFlow。由于框架规模、依赖环境的限制，这些框架不适合在手机、开发板等生产环境中安装。 
2. 深度学习模型的结构通常比较庞大，需要大量的算力才能满足实时运行的需求。模型的运行效率需要优化。 因为这些难题的存在，模型部署不能靠简单的环境配置与安装完成。

经过工业界和学术界数年的探索，结合`XEdu`的工具，下图展示了模型部署的一条流行流水线：

![](D:\XEdu-docs\source\images\model_convert\XEduPipeline.JPG)


这条流水线解决了模型部署中的两大问题：使用对接深度学习框架和推理引擎的中间表示。开发者不必担心如何在新环境中运行各个复杂的框架；通过中间表示的网络结构优化和推理引擎对运算的底层优化，使模型的运算效率大幅提升。

## 三、如何进行模型转换？

使用MMEdu、BaseNN的convert函数进行一键式模型转换。

### 1.MMEdu模型转换

MMEdu内置了一个`convert`函数，来实现了一键式模型转换，转换前先了解一下转换要做的事情吧。

- 转换准备：

  待转换的模型权重文件（用MMEdu训练）。

- 需要配置两个信息：

  待转换的模型权重文件（`checkpoint`）和输出的文件（`out_file`）。

- 模型转换的典型代码：

```
from MMEdu import MMClassification as cls
model = cls(backbone='MobileNet')
checkpoint = 'checkpoints/cls_model/CatsDog/best_accuracy_top-1_epoch_2.pth'
out_file="catdog.onnx"
model.convert(checkpoint=checkpoint, out_file=out_file)
```

这段代码是完成分类模型的转换，接下来对为您`model.convert`函数的各个参数：

`checkpoint`：选择想要进行模型转换的权重文件，以.pth为后缀。

`out_file`：模型转换后的输出文件路径。


类似的，目标检测模型转换的示例代码如下：

```
from MMEdu import MMDetection as det
model = det(backbone='SSD_Lite')
checkpoint = 'checkpoints/COCO-80/ssdlite.pth'
out_file="COCO-80.onnx"
model.convert(checkpoint=checkpoint, out_file=out_file)
```

参考项目：<a href="https://www.openinnolab.org.cn/pjlab/project?id=645110943c0e930cb55e859b&sc=62f34141bf4f550f3e926e0e#public">MMEdu模型转换
</a>

## 2.BaseNN模型转换

BaseNN内置了一个`convert`函数，来实现了一键式模型转换，转换前先了解一下转换要做的事情吧。

- 转换准备：

  待转换的模型权重文件（用BaseNN训练）。

- 需要配置两个信息：

  待转换的模型权重文件（`checkpoint`）和输出的文件（`out_file`）。

- 模型转换的典型代码：

```python
from BaseNN import nn
model = nn()
model.convert(checkppint="basenn_cd.pth",out_file="basenn_cd.onnx")
```

`model.convert()`参数信息：

`checkpoint`: 指定要转换的pth模型文件路径

`out_file`: 指定转换出的onnx模型文件路径

## 四、如何进行模型部署？

使用转换后生成的示例代码和ONNX模型，可轻松将模型部署到硬件设备上。示例代码按照不同的应用场景和需求进行分类。

### 1.基础版

这个版本的示例代码主要展示如何将转换后的模型在不同硬件设备上进行基本的推理任务。这个版本适用于初学者和对模型转换和部署有一定了解的用户。

### 2.加摄像头版

这个版本的示例代码将展示如何在模型转换后，结合摄像头进行实时图像识别。这个版本适用于希望在实际场景中应用模型的用户，例如监控系统、智能交通等。

## 五、模型在不同硬件设备上的性能表现

本文提供了在不同硬件设备（如PC、行空板、树莓派等）上部署模型的测试结果，包括模型大小、准确率、推理速度等指标。

## 六、相关项目与资源

本文还提供了一些相关的项目和资源，如猫狗分类小助手、千物识别小助手等，以及模型在线转换工具。

希望这个梳理对您有所帮助。如果您需要进一步的解释或有其他问题，请随时告诉我。